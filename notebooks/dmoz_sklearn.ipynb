{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json_lines, json, gzip, csv\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('../../dmoz/content_url_topics.csv.gz', 'rt') as f:\n",
    "    topics_by_url = dict(csv.reader(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs_topics = []\n",
    "def gen(limit=None):\n",
    "    xs_topics[:] = []\n",
    "    with json_lines.open('../dmoz-text.jl.gz') as f:\n",
    "        items = f if limit is None else islice(f, limit)\n",
    "        for item in items:\n",
    "            xs_topics.append(topics_by_url.get(item['url'], ''))\n",
    "            yield item['text']\n",
    "\n",
    "vec = TfidfVectorizer(max_features=200000)\n",
    "all_xs = vec.fit_transform(gen(limit=100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 95000)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(xs_topics)\n",
    "indices = np.arange(N)\n",
    "np.random.shuffle(indices)\n",
    "all_xs = all_xs[indices]\n",
    "xs_topics = [xs_topics[idx] for idx in indices]\n",
    "\n",
    "n_valid = min(5000, int(N * 0.2))\n",
    "xs_valid = all_xs[:n_valid]\n",
    "xs_train = all_xs[n_valid:]\n",
    "\n",
    "all_ys = [[t for t in item_topics.split('/')[1:]\n",
    "           if not (len(t) == 1 and t.isupper())]\n",
    "          for item_topics in xs_topics]\n",
    "n_topics = 1000\n",
    "topic_counts = Counter(t for item_topics in all_ys for t in item_topics)\n",
    "most_common_topics = {t for t, _ in topic_counts.most_common(n_topics)}\n",
    "all_ys = [[t for t in item_topics if t in most_common_topics]\n",
    "          for item_topics in all_ys]\n",
    "\n",
    "ys_valid = all_ys[:n_valid]\n",
    "ys_train = all_ys[n_valid:]\n",
    "\n",
    "len(ys_valid), len(ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('World', 46353),\n",
       " ('Regional', 34901),\n",
       " ('North_America', 19029),\n",
       " ('United_States', 18497),\n",
       " ('Localities', 14090),\n",
       " ('Deutsch', 12404),\n",
       " ('Europa', 11644),\n",
       " ('Europe', 9831),\n",
       " ('Business_and_Economy', 8732),\n",
       " ('Français', 6442)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artes_escénicas', 62),\n",
       " ('Zuhause', 62),\n",
       " ('Localităţi', 62),\n",
       " ('Antwerpen', 62),\n",
       " ('Religia_i_duchowość', 61),\n",
       " ('Zeitschriften_und_Online-Magazine', 61),\n",
       " ('Kultur_und_Unterhaltung', 61),\n",
       " ('Winter_Sports', 61),\n",
       " ('Zakupy', 61),\n",
       " ('Services_aux_entreprises', 61)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counts.most_common(n_topics)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_ys(ys):\n",
    "    return [label_ids[random.choice(item_topics) if item_topics else no_topic]\n",
    "            for item_topics in ys]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def get_log_loss(xs, ys): \n",
    "    return log_loss(get_ys(ys), clf.predict_proba(xs), labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train log loss: 5.23, valid log loss: 5.25\n",
      "Epoch 2: train log loss: 5.18, valid log loss: 5.20\n",
      "Epoch 3: train log loss: 5.16, valid log loss: 5.16\n",
      "Epoch 4: train log loss: 5.15, valid log loss: 5.16\n",
      "Epoch 5: train log loss: 5.12, valid log loss: 5.18\n",
      "Epoch 6: train log loss: 5.14, valid log loss: 5.16\n",
      "Epoch 7: train log loss: 5.15, valid log loss: 5.14\n",
      "Epoch 8: train log loss: 5.11, valid log loss: 5.12\n",
      "Epoch 9: train log loss: 5.11, valid log loss: 5.14\n",
      "Epoch 10: train log loss: 5.10, valid log loss: 5.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "no_topic = 'no_topic'\n",
    "assert no_topic not in most_common_topics\n",
    "labels = sorted(most_common_topics)\n",
    "labels.append(no_topic)\n",
    "label_ids = {label: idx for idx, label in enumerate(labels)}\n",
    "classes = sorted(label_ids.values())\n",
    "\n",
    "clf = SGDClassifier(loss='log', n_jobs=-1)\n",
    "for epoch in range(10):\n",
    "    clf.partial_fit(xs_train, get_ys(ys_train), classes=classes)\n",
    "    print('Epoch {epoch}: train log loss: {train:.2f}, valid log loss: {valid:.2f}'.format(\n",
    "        epoch=epoch + 1,\n",
    "        train=get_log_loss(xs_train[:n_valid], ys_train[:n_valid]),\n",
    "        valid=get_log_loss(xs_valid, ys_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(vec, clf)\n",
    "with open('../dmoz_sklearn.pkl', 'wb') as f:\n",
    "    pickle.dump({'pipeline': pipeline, 'labels': labels}, f,\n",
    "                protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 kostia kostia 1,6G дек  8 14:15 ../dmoz_sklearn.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh ../dmoz_sklearn.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
